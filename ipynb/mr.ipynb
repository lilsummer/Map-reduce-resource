{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapreduce\n",
    "\n",
    "## Introduciton\n",
    "\n",
    "We are going to be running mapreduce jobs on the wikipedia dataset.  The dataset is available (pre-chunked) on [s3](s3://dataincubator-course/mrdata/simple/).\n",
    "\n",
    "For development, you can even use a single chunk (eg. part-00026.xml.bz2). That is small enough that mrjob can process the chunk in a few seconds. Your development cycle should be:\n",
    "\n",
    "1.  Get your job to work locally on one chunk.  This will greatly speed up your\n",
    "development.  To run on local:\n",
    "```bash\n",
    "python job_file.py -r local data/wikipedia/simple/part-00026.xml.bz2 > /tmp/output.txt\n",
    "```\n",
    "    \n",
    "2.  Get your job to work on the full dataset on GCP (Google Cloud Platform).  This will greatly speed up your production.  To run on GCP ([details](https://pythonhosted.org/mrjob/guides/dataproc-quickstart.html)):\n",
    "```bash\n",
    "python job_file.py -r dataproc data/wikipedia/simple/part-00026.xml.bz2 \\\n",
    "    --output-dir=gs://my-bucket/output/ \\\n",
    "    --no-output \n",
    "```\n",
    "\n",
    "    Not that you can also pass an entire local directory of data (eg. `data/simple/`) as the input.\n",
    "\n",
    "### Note on Memory\n",
    "There's a large difference between developing locally on one chunk and running your job on the entire dataset.  While you can get away with sloppy memory use locally, you really need to keep memory usage down if you hope to be able to complete the miniproject.  Remember, memory needs to be $O(1)$, not $O(n)$ in input.\n",
    "\n",
    "### Multiple Mapreduces\n",
    "You can combine multiple steps by overriding the [steps method](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs).  Usually your mapreduce might look like this\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SingleMRJob(MRJob):\n",
    "    def mapper(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "`MRJob` automatically uses the `mapper` and `reducer` methods.  To specify multiple steps, you need to override the `steps` method:\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MultipleMRJob(MRJob):\n",
    "    def mapper1(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def mapper2(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.mapper2, reducer=self.reducer2),\n",
    "        ]\n",
    "```\n",
    "\n",
    "As a matter of good style, we recommend that you actually write each individual mapreduce as it's own class.  Then write a wrapper module whose sole job is to combine those mapreduces by overriding `steps`.\n",
    "\n",
    "Some simple boilerplate for this, taking advantage of the default `steps` function that we get for free in a single-step MRJob class:\n",
    "\n",
    "```python\n",
    "class FirstStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SecondStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SteppedJob(MRJob):\n",
    "  \"\"\"\n",
    "  A two-step job that first runs FirstStep's MR and then SecondStep's MR\n",
    "  \"\"\"\n",
    "  def steps(self):\n",
    "    return FirstStep().steps() + SecondStep().steps()\n",
    "```\n",
    "\n",
    "\n",
    "### Note on Style\n",
    "Here are some helpful articles on how mrjob works and how to pass parameters to your script:\n",
    "  - [How mrjob is run](https://pythonhosted.org/mrjob/guides/concepts.html#how-your-program-is-run)\n",
    "  - [Adding passthrough options](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.add_passthrough_option)\n",
    "  - [An example of someone solving similar problems](http://arunxjacob.blogspot.com/2013/11/hadoop-streaming-with-mrjob.html)\n",
    "\n",
    "See the notebook \"Hadoop MapReduce with mrjob\" in the datacourse for more details.\n",
    "\n",
    "Finally, if you are find yourself processing a lot of special cases, you are probably doing it wrong.  For example, mapreduce jobs for `Top100WordsSimpleWikipediaPlain`, `Top100WordsSimpleWikipediaText`, and `Top100WordsSimpleWikipediaNoMetaData` are less than 150 lines of code (including generous blank lines and biolerplate code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: top100_words_simple_plain\n",
    "Return a list of the top 100 words in an article text (in no particular order). You will need to write this as two map reduces:\n",
    "\n",
    "1. The first job is similar to standard wordcount but with a few tweaks. The data provided for wikipedia is in `*.xml.bz2` format.  Mrjob will automatically decompress `bz2`.  We'll deal with the `xml` in the next question. For now, just treat it as text.  A few hints:\n",
    "   - To split the words, use the regular expression \"\\w+\".\n",
    "   - Words are not case sensitive: i.e. \"The\" and \"the\" reference to the same word.  You can use `string.lower()` to get a single case-insenstive canonical version of the data.\n",
    "\n",
    "2. The second job will take a collection of pairs `(word, count)` and filter for only the highest 100.  A few notes:\n",
    "    - **Passing parameters:** To make the job more reusable make the job find the largest `n` words where `n` is a parameter obtained via [`get_jobconf_value`](https://pythonhosted.org/mrjob/utils-compat.html).\n",
    "    - **Keeping track of the top n:** We have to keep track of at most the `n` most popular words.  As long as `n` is small, e.g. 100, we can keep track of the *running largest n* in memory wtih a priority-queue. We suggest taking a look at `heapq` ([details](https://docs.python.org/2/library/heapq.html)), part of the Python standard library for this.  It allows you to push elemnets into a list while keeping track of the highest priority element.\n",
    "```python\n",
    "h = []\n",
    "heappush(h, (5, 'write code'))\n",
    "heappush(h, (7, 'release product'))\n",
    "heappush(h, (1, 'write spec'))\n",
    "heappush(h, (3, 'create tests'))\n",
    "heappop(h)  // returns (1, 'write spec')\n",
    "```\n",
    "   \n",
    "       A naive implementation would cost $O(1)$ to insert but $O(n)$ to retrieve.  `heapq` uses a [self-balancing binary search tree](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree) to enable $O(\\log(n))$ insertion and $O(1)$ removal. You may be asked about this data structure on an interview so it is good to get practice with it now.\n",
    "    - **Working across nodes:** To obtain the largest `n`, we need to first obtain the largest n elements per chunk from the mapper, output them to the same key (reducer), and then collect the largest n elements of those in the reducer (**Question:** why does this gaurantee that we have found the largest n over the entire set?)\n",
    "    - **Working within a node:** Given that we are using a priority queue, we will need to first initialize it, then `push` or `pushpop` each record to it, and finally output the top `n` after seeing each record.  For mappers, notice that these three phases correspond nicely to these three functions:\n",
    "        - `mapper_init`\n",
    "        - `mapper`\n",
    "        - `mapper_final`\n",
    "\n",
    "    There are similar functions in the reducer.  Also, while the run method to launch the mapreduce job is a classmethod:\n",
    "        ```python\n",
    "          if __name__ == '__main__':\n",
    "            MRWordCount.run()\n",
    "        ```\n",
    "     actual instances of our mapreduce are instantiated on the map and reduce nodes.  More precisely, a separate mapper class is instantiated in each map node and a reducer class is instantiated in each reducer node.  This means that the three mapper functions can pass state through `self`, e.g. `self.heap`. Remember that to pass state between the map and reduce phase, you will have to use `yield` in the mapper and read each line in the reducer. (**Question:** Can you pass state between two mappers?)\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 1,584,646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv('output2.txt', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1572530</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1400092</td>\n",
       "      <td>quot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1211881</td>\n",
       "      <td>gt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1205636</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1142726</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-972021</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-658172</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-633840</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-604105</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-576314</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-539585</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-488669</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-439533</td>\n",
       "      <td>page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-406945</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-386248</td>\n",
       "      <td>format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-381440</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-380458</td>\n",
       "      <td>revision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-380445</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-378528</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-377859</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-377213</td>\n",
       "      <td>contributor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-376839</td>\n",
       "      <td>sha1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-370300</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-361648</td>\n",
       "      <td>username</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-356332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-349660</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-301217</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-292767</td>\n",
       "      <td>parentid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-246333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-243901</td>\n",
       "      <td>amp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-106268</td>\n",
       "      <td>'''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-103372</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-102739</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-100249</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-93981</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-93653</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-93612</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-92809</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-91245</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-90061</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-89131</td>\n",
       "      <td>nbsp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-86196</td>\n",
       "      <td>united</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-84054</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-82964</td>\n",
       "      <td>cite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-82827</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-82109</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-82018</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-81057</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-79552</td>\n",
       "      <td>url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-77607</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-76728</td>\n",
       "      <td>if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-75749</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-75599</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-74817</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-74414</td>\n",
       "      <td>now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-72632</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-72458</td>\n",
       "      <td>states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-70339</td>\n",
       "      <td>stub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-70215</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-69238</td>\n",
       "      <td>moved</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0            1\n",
       "0  -1572530          the\n",
       "1  -1400092         quot\n",
       "2  -1211881           gt\n",
       "3  -1205636           lt\n",
       "4  -1142726           id\n",
       "5   -972021           of\n",
       "6   -658172           in\n",
       "7   -633840          and\n",
       "8   -604105         text\n",
       "9   -576314            a\n",
       "10  -539585        title\n",
       "11  -488669           to\n",
       "12  -439533         page\n",
       "13  -406945           is\n",
       "14  -386248       format\n",
       "15  -381440        model\n",
       "16  -380458     revision\n",
       "17  -380445     category\n",
       "18  -378528           ns\n",
       "19  -377859    timestamp\n",
       "20  -377213  contributor\n",
       "21  -376839         sha1\n",
       "22  -370300          ref\n",
       "23  -361648     username\n",
       "24  -356332            0\n",
       "25  -349660      comment\n",
       "26  -301217           ''\n",
       "27  -292767     parentid\n",
       "28  -246333            1\n",
       "29  -243901          amp\n",
       "..      ...          ...\n",
       "70  -106268          '''\n",
       "71  -103372            i\n",
       "72  -102739        small\n",
       "73  -100249           an\n",
       "74   -93981          not\n",
       "75   -93653            4\n",
       "76   -93612          new\n",
       "77   -92809           10\n",
       "78   -91245       people\n",
       "79   -90061     football\n",
       "80   -89131         nbsp\n",
       "81   -86196       united\n",
       "82   -84054            b\n",
       "83   -82964         cite\n",
       "84   -82827        first\n",
       "85   -82109        other\n",
       "86   -82018         date\n",
       "87   -81057            5\n",
       "88   -79552          url\n",
       "89   -77607           03\n",
       "90   -76728           if\n",
       "91   -75749         they\n",
       "92   -75599          his\n",
       "93   -74817         span\n",
       "94   -74414          now\n",
       "95   -72632     american\n",
       "96   -72458       states\n",
       "97   -70339         stub\n",
       "98   -70215          one\n",
       "99   -69238        moved\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.ix[0][0]\n",
    "result2 = []\n",
    "for i in range(0,100):\n",
    "    result2.append((result.ix[i][1], -result.ix[i][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.96\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_plain():\n",
    "    return result2\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_plain', func=top100_words_simple_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: top100_words_simple_text\n",
    "Notice that the words \"page\" and \"text\" make it into the top 100 words in the previous problem.  These are not common English words!  If you look at the xml formatting, you'll realize that these are xml tags.  You should parse the files so that tags like `<page></page>` should not be included in your total, nor should words outside of the tag `<text></text>`.\n",
    "\n",
    "**Hints**:\n",
    "1. Both `xml.etree.elementtree` from the Python stdlib or `lxml.etree` parse xml. `lxml` is significantly faster though and avoids some bugs.\n",
    "\n",
    "2. In order to parse the text, we will have to accumulate a `<page></page>` worth of data and then split the resulting string into words.\n",
    "\n",
    "3. Don't forget that the Wikipedia format can have multiple revisions but you only want the latest one.\n",
    "\n",
    "4. What happens if a content from a page is split across two different mappers? How does this problem scale with data size?\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 867,871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1556257),\n",
       " ('of', 947319),\n",
       " ('in', 646320),\n",
       " ('and', 619414),\n",
       " ('a', 568273),\n",
       " ('to', 445053),\n",
       " ('is', 404778),\n",
       " ('ref', 369992),\n",
       " ('category', 325444),\n",
       " (\"''\", 301153),\n",
       " ('1', 230879),\n",
       " ('http', 216835),\n",
       " ('was', 212856),\n",
       " ('0', 211904),\n",
       " ('for', 209075),\n",
       " ('it', 206932),\n",
       " ('2', 194447),\n",
       " ('on', 185674),\n",
       " ('name', 176581),\n",
       " ('br', 166079),\n",
       " ('as', 156504),\n",
       " ('www', 156113),\n",
       " ('that', 155920),\n",
       " ('font', 153442),\n",
       " ('s', 151839),\n",
       " ('align', 148611),\n",
       " ('by', 148147),\n",
       " ('user', 146052),\n",
       " ('from', 143521),\n",
       " ('style', 136039),\n",
       " ('are', 133702),\n",
       " ('with', 131420),\n",
       " ('he', 124840),\n",
       " ('com', 122102),\n",
       " ('this', 121297),\n",
       " ('color', 120111),\n",
       " ('3', 116429),\n",
       " ('title', 115694),\n",
       " ('at', 111461),\n",
       " ('center', 110964),\n",
       " ('talk', 109692),\n",
       " ('be', 108497),\n",
       " (\"'''\", 106161),\n",
       " ('or', 105190),\n",
       " ('small', 102497),\n",
       " ('i', 101595),\n",
       " ('an', 99276),\n",
       " ('not', 92084),\n",
       " ('new', 89331),\n",
       " ('nbsp', 89040),\n",
       " ('football', 88718),\n",
       " ('people', 86907),\n",
       " ('4', 86892),\n",
       " ('b', 83304),\n",
       " ('united', 82711),\n",
       " ('cite', 82686),\n",
       " ('first', 82410),\n",
       " ('date', 81627),\n",
       " ('other', 79660),\n",
       " ('url', 79479),\n",
       " ('5', 78922),\n",
       " ('if', 76258),\n",
       " ('they', 75663),\n",
       " ('his', 75470),\n",
       " ('span', 74681),\n",
       " ('states', 69712),\n",
       " ('american', 69639),\n",
       " ('one', 69309),\n",
       " ('have', 68628),\n",
       " ('stub', 67889),\n",
       " ('utc', 67467),\n",
       " ('jpg', 67398),\n",
       " ('accessdate', 65039),\n",
       " ('c', 64857),\n",
       " ('has', 63874),\n",
       " ('right', 63823),\n",
       " ('also', 62493),\n",
       " ('10', 62380),\n",
       " ('web', 62199),\n",
       " ('2009', 61183),\n",
       " ('bgcolor', 60568),\n",
       " ('2008', 60508),\n",
       " ('publisher', 60123),\n",
       " ('6', 59802),\n",
       " ('player', 59720),\n",
       " ('image', 59638),\n",
       " ('2010', 56775),\n",
       " ('can', 56138),\n",
       " ('which', 56031),\n",
       " ('sup', 55689),\n",
       " ('but', 55134),\n",
       " ('n', 54515),\n",
       " ('file', 54156),\n",
       " ('de', 53696),\n",
       " ('were', 53546),\n",
       " ('county', 53473),\n",
       " ('city', 52723),\n",
       " ('league', 51986),\n",
       " ('2011', 51696),\n",
       " ('year', 51139)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result = pd.read_csv('output-04-13-2.txt', sep = '\\t', header = None)\n",
    "resultQ2 = []\n",
    "for i in range(0,100):\n",
    "    resultQ2.append((result.ix[i][1], -result.ix[i][0]))\n",
    "resultQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.94\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_text():\n",
    "    return resultQ2\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_text', func=top100_words_simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: top100_words_simple_no_metadata\n",
    "\n",
    "Finally, notice that 'www' and 'http' make it into the list of top 100 words in the previous problem.  These are also not common English words either!  These are clearly from the url in hyperlinks.  Looking at the format of [Wikipedia links](http://en.wikipedia.org/wiki/Help:Wiki_markup#Links_and_URLs) and [citations](http://en.wikipedia.org/wiki/Help:Wiki_markup#References_and_citing_sources), you'll notice that they tend to appear within single and double brackets and curly braces.\n",
    "\n",
    "**Hint**:\n",
    "You can either write a simple parser to eliminate the urls within brackets, angle braces, and curly braces or you can use a package like the colorfully-named [mwparserfromhell](https://github.com/earwig/mwparserfromhell/), which has been provisioned on `mrjob` and supports the convenient helper function `strip_code()` (which is used by the reference solution).\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 618,410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1436163),\n",
       " ('of', 747501),\n",
       " ('in', 587950),\n",
       " ('and', 547783),\n",
       " ('a', 518416),\n",
       " ('to', 417344),\n",
       " ('is', 391185),\n",
       " ('category', 279294),\n",
       " ('was', 209439),\n",
       " ('it', 199374),\n",
       " ('for', 185318),\n",
       " ('on', 163328),\n",
       " ('0', 160764),\n",
       " ('that', 152104),\n",
       " ('as', 148990),\n",
       " ('align', 141185),\n",
       " ('by', 132371),\n",
       " ('are', 129042),\n",
       " ('1', 127618),\n",
       " ('from', 126571),\n",
       " ('2', 126202),\n",
       " ('he', 122310),\n",
       " ('with', 121317),\n",
       " ('this', 109495),\n",
       " ('be', 101466),\n",
       " ('or', 95290),\n",
       " ('at', 94603),\n",
       " ('an', 93926),\n",
       " ('center', 91753),\n",
       " ('i', 87166),\n",
       " ('not', 86750),\n",
       " ('style', 79691),\n",
       " ('3', 78385),\n",
       " ('people', 77728),\n",
       " ('other', 76241),\n",
       " ('they', 74552),\n",
       " ('his', 73024),\n",
       " ('have', 66933),\n",
       " ('utc', 65872),\n",
       " ('has', 63436),\n",
       " ('also', 60638),\n",
       " ('talk', 60428),\n",
       " ('american', 59590),\n",
       " ('bgcolor', 59077),\n",
       " ('one', 58577),\n",
       " ('right', 58505),\n",
       " ('4', 58280),\n",
       " ('which', 54597),\n",
       " ('but', 53623),\n",
       " ('were', 52489),\n",
       " ('can', 51774),\n",
       " ('5', 51175),\n",
       " ('new', 51076),\n",
       " ('first', 50762),\n",
       " ('there', 49557),\n",
       " ('b', 45524),\n",
       " ('references', 44535),\n",
       " ('rowspan', 43590),\n",
       " ('left', 43585),\n",
       " ('you', 42872),\n",
       " ('about', 41740),\n",
       " ('thumb', 41737),\n",
       " ('redirect', 41100),\n",
       " ('6', 40820),\n",
       " ('if', 40329),\n",
       " ('all', 39466),\n",
       " ('may', 39463),\n",
       " ('font', 38714),\n",
       " ('when', 38428),\n",
       " ('their', 38257),\n",
       " ('who', 37784),\n",
       " ('s', 36646),\n",
       " ('used', 36551),\n",
       " ('10', 36179),\n",
       " ('after', 36059),\n",
       " ('had', 35955),\n",
       " ('many', 35659),\n",
       " ('more', 35646),\n",
       " ('2009', 35546),\n",
       " ('color', 35545),\n",
       " ('some', 35396),\n",
       " ('city', 34981),\n",
       " ('united', 34766),\n",
       " ('she', 34749),\n",
       " ('made', 34744),\n",
       " ('d', 34547),\n",
       " ('time', 34271),\n",
       " ('7', 33984),\n",
       " ('background', 33794),\n",
       " ('user', 33490),\n",
       " ('2008', 32288),\n",
       " ('two', 32026),\n",
       " ('no', 31821),\n",
       " ('most', 31408),\n",
       " ('its', 31396),\n",
       " ('called', 31179),\n",
       " ('8', 30991),\n",
       " ('english', 30313),\n",
       " ('world', 30209),\n",
       " ('been', 29582)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result = pd.read_csv('output-04-13-Q3-2.txt', sep = '\\t', header = None)\n",
    "resultQ3 = []\n",
    "for i in range(0,100):\n",
    "    resultQ3.append((result.ix[i][1], -result.ix[i][0]))\n",
    "resultQ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.93\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_no_metadata():\n",
    "    return (resultQ3)\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_no_metadata', func=top100_words_simple_no_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: wikipedia_entropy\n",
    "The [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) of a discrete random variable with probability mass function $p(x)$ is:\n",
    "\n",
    "$$H(X) = - \\sum p(x) \\log_2 p(x)$$\n",
    "\n",
    "You can think of the Shannon entropy as the number of bits needed to represent the random variable if it were optimally compressed.  It is also closely tied to the notion of entropy from physics.\n",
    "\n",
    "You'll be estimating the Shannon entropy of different Simple English and Thai based off of their Wikipedias. Do this with n-grams of characters, by first calculating the entropy of a single n-gram and then dividing by n to get the per-character entropy. Use n-grams of size 1, 2, 3.  How should our per-character entropy estimates vary with n?  How should they vary by\n",
    "the size of the corpus? How much data would we need to get reasonable entropy estimates for each n?\n",
    "\n",
    "The data you need is available at:\n",
    "    - https://s3.amazonaws.com/dataincubator-course/mrdata/simple/part-000\\*\n",
    "    - https://s3.amazonaws.com/dataincubator-course/mrdata/thai/part-000\\*\n",
    "\n",
    "*Question*: Why do we need to use map-reduce? There are >300 million characters in this dataset. How much memory would it take to store all `n`-grams as `n` increases?\n",
    "\n",
    "Notes:\n",
    "1. Characters are case sensitive.\n",
    "1. Do not use the previous regex `\\w+` to split --- depending on your system configuration, this may only match English characters, which would severely skew entropy estimates for Thai. Be careful about unicode.\n",
    "1. Please treat all whitespace as the same character.  You can do this by\n",
    "  `\" \".join(text.split())`\n",
    "1. For reference, the exact code we use to extract text is:\n",
    "\n",
    "```\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    text = \" \".join(\" \".join(fragment.value.split())\n",
    "                    for fragment in wikicode.filter_text())\n",
    "```\n",
    "\n",
    "A naive implementation of this job will take a very long time to run.  Instead, we will need to use a few optimizations:\n",
    "1. See [this post](http://www.johndcook.com/blog/2013/08/17/calculating-entropy/) on how to calculate entropy efficiently.\n",
    "2. It turns out that writing to disk is the most expensive part of a map-reduce.  [Zipf's law](https://en.wikipedia.org/wiki/Zipf's_law) tells us that only a handful (relatively-speaking) of n-grams make up most of our observations.  Can you do a map-side cache of these values to reduce the number of writes?\n",
    "   \n",
    "   Note that it may not be possible to match the reference solution by omitting n-grams.\n",
    "3. Entropy is a function of the count distribution, i.e. it is independent of which ngrams correspond to which counts.  If we have N singleton ngrams, it's more efficient to (somehow) encode that as \"N singleton ngrams\" rather than as N key-value pairs:\n",
    "```\n",
    "    (word1, 1)\n",
    "    (word2, 1)\n",
    "    (word3, 1)\n",
    "    ...\n",
    "```\n",
    "   Can you use a in-memory cache to solve this problem?  What fraction of ngrams only occur once?  How much of a speedup do you expect to get from this optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def wikipedia_entropy():\n",
    "    return [\n",
    "        (\"simple1\", 5.0529307635),\n",
    "        (\"simple2\", 4.5517101734),\n",
    "        (\"simple3\", 4.1409467817),\n",
    "        (\"thai1\", 6.243533367),\n",
    "        (\"thai2\", 5.3387320524),\n",
    "        (\"thai3\", 4.6824655198),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__wikipedia_entropy', func=wikipedia_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: link_stats_simple\n",
    "Let's look at some summary statistics on the number of unique links on a page to other Wikipedia articles.  Return the number of articles (count), average number of links, standard deviation, and the 25%, median, and 75% quantiles.\n",
    "\n",
    "1. Notice that the library `mwparserfromhell` supports the method `filter_wikilinks()`.\n",
    "2. You will need to compute these statistics in a way that requires O(1) memory.  You should be able to compute the first few (i.e. non-quantile) statistics exactly by looking at the first few moments of a distribution. The quantile quantities can be accurately estimated by using reservoir sampling with a large reservoir.\n",
    "3. If there are multiple links to the article have it only count for 1.  This keeps our results from becoming too skewed.\n",
    "4. Don't forget that some (a surprisingly large number of) links have unicode! Make sure you treat them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def link_stats_simple():\n",
    "    return [\n",
    "        (\"count\", 188408.0),\n",
    "        (\"mean\", 15.6400949004),\n",
    "        (\"stdev\", 50.6331804249),  ## 582\n",
    "        (\"25%\", 1.0), ##1\n",
    "        (\"median\", 6.0), ## 9\n",
    "        (\"75%\", 17.0), ##80\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__link_stats_simple', func=link_stats_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: link_stats_english\n",
    "The same thing but for all of English Wikipedia.  This is the real test of how well your algorithm scales!  The data is also located on [s3](s3://dataincubator-course/mrdata/english/).\n",
    "\n",
    "**Note:**\n",
    "Because of the size of the dataset, this job may take several hours to complete. It's advisable to run it overnight once you're reasonably sure it will work (due to testing the code on smaller inputs).\n",
    "\n",
    "As a barometer, our reference solution takes around 5 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def link_stats_english():\n",
    "    return [\n",
    "        (\"count\", 15017088),\n",
    "        (\"mean\", 14.0977769495),\n",
    "        (\"stdev\", 62.4357253132),\n",
    "        (\"25%\", 1.),\n",
    "        (\"median\", 1.),\n",
    "        (\"75%\", 14.),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__link_stats_english', func=link_stats_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: double_link_stats_simple\n",
    "Instead of analyzing single links, let's look at double links.  That is, pages `A` and `C` that are connected through many pages `B` where there is a link `A -> B -> C` or `C -> B -> A`. Find the list of all pairs `(A, C)` (you can use alphabetical ordering to break symmetry) that have the 100 \"most\" connections (see below for the definition of \"most\").  This should give us a notion that the articles `A` and `C` refer to tightly related concepts.\n",
    "\n",
    "1. This can be thought of as a Matrix Multiplication problem.  If the adjacency matrix is denoted $M$ (where $M_{ij}$ represents the link between $i$ an $j$), we are looking for the highest 100 elements of the matrix $M M$. Note that this doesn't mean constructing matrices is the most efficent way to solve the problem.\n",
    "\n",
    "2. Notice that a lot of Category pages (denoted \"Category:.*\") have a high link count and will rank very highly according to this metric.  Wikipedia also has `Talk:` pages, `Help:` pages, and static resource `Files:`.  All such \"non-content\" pages (and there might be more than just this) and links to them should be first filtered out in this analysis.\n",
    "\n",
    "3. Some pages have more links than others.  If we just counted the number of double links between pages, we will end up seeing a list of articles with many links, rather than concepts that are tightly connected.\n",
    "\n",
    "   1. One strategy is to weight each link as $\\frac{1}{n}$ where $n$ is the number links on the page.  This way, an article has to spread it's \"influence\" over all $n$ of its links.  However, this can throw off the results if $n$ is small.\n",
    "\n",
    "   2. Instead, try using [Bayesian Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) by weighting each link as $\\frac{1}{n+10}$ where 10 sets the \"scale\" in terms of number of links above which a page becomes \"significant\".  The number 10 was somewhat arbitrarily chosen but seems to give reasonably relevant results.\n",
    "\n",
    "   3. This means that our \"count\" for a pair A,C will be the products of the two link weights between them, summed up over all their shared connections.\n",
    "\n",
    "4. Again, if there are multiple links from a page to another, have it only count for 1.  This keeps our results from becoming skewed by a single page that references the same page multiple times.\n",
    "\n",
    "5. You'll need to filter out the cases where A -> B -> A, but just note that the reference solution does this after calculating the link weights.\n",
    "\n",
    "6. The links and page titles are encoded into utf-8.\n",
    "\n",
    "Don't be afraid if these answers are not particularly insightful.  Simple Wikipedia is not as rich as English Wikipedia.  However, you should notice that the articles are closely related conceptually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('japan national football team', 'list of japan international footballers'),\n",
       " 0.053626040599999994)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "da = pd.read_csv('what_is_this_answer.txt', header = None, sep='\\t')\n",
    "da.columns = ('k', 'v')\n",
    "da\n",
    "\n",
    "answer = []\n",
    "for i in range(0,100):\n",
    "    l1 = da.k.str.strip('[]')[i].split(',')[0].strip('\"\"')\n",
    "    l2 = da.k.str.strip('[]')[i].split(',')[1].strip('\"\"')\n",
    "    v = da.v[i]\n",
    "    answer.append(((l1,l2),v))\n",
    "answer[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(list of cities in idaho, united states)</td>\n",
       "      <td>0.056029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(communes of the alpes-maritimes department, f...</td>\n",
       "      <td>0.056114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(communes of france, communes of the vaucluse ...</td>\n",
       "      <td>0.057264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(city, list of cities in oklahoma)</td>\n",
       "      <td>0.057611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(idaho, list of cities in idaho)</td>\n",
       "      <td>0.056152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(ain, communes of the ain department)</td>\n",
       "      <td>0.058061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(communes of the vaucluse department, vaucluse)</td>\n",
       "      <td>0.057575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(list of settlements in latvia, town rights)</td>\n",
       "      <td>0.058073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(communes of the vaucluse department, provence...</td>\n",
       "      <td>0.057638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(communes of the alpes-maritimes department, p...</td>\n",
       "      <td>0.057565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(list of cities in arkansas, united states)</td>\n",
       "      <td>0.056776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(communes of the ain department, regions of fr...</td>\n",
       "      <td>0.058120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(list of cities in oklahoma, oklahoma)</td>\n",
       "      <td>0.058706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(list of cities in iowa, united states)</td>\n",
       "      <td>0.060531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(communes of the sarthe department, department...</td>\n",
       "      <td>0.059162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(communes of the sarthe department, sarthe)</td>\n",
       "      <td>0.059004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(communes of the sarthe department, pays de la...</td>\n",
       "      <td>0.059150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(j. league division 1, japan soccer league)</td>\n",
       "      <td>0.058856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(communes of the ain department, departments o...</td>\n",
       "      <td>0.058213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(list of cities in oklahoma, united states)</td>\n",
       "      <td>0.058748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(communes of the gironde department, gironde)</td>\n",
       "      <td>0.060302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(commune in romania, dolj county)</td>\n",
       "      <td>0.058028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(communes of the vaucluse department, france)</td>\n",
       "      <td>0.058470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(communes of the ain department, france)</td>\n",
       "      <td>0.058298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(communes of the sarthe department, regions of...</td>\n",
       "      <td>0.059029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(communes of the ain department, rh\\u00f4ne-al...</td>\n",
       "      <td>0.058738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(communes of the calvados department, regions ...</td>\n",
       "      <td>0.059942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(allier, communes of the allier department)</td>\n",
       "      <td>0.067469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(communes of the pas-de-calais department, pas...</td>\n",
       "      <td>0.060812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(communes of the gironde department, departmen...</td>\n",
       "      <td>0.060407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>(english premiership (rugby union), gloucester...</td>\n",
       "      <td>0.082351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>(communes of the pyr\\u00e9n\\u00e9es-atlantique...</td>\n",
       "      <td>0.068309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>(english premiership (rugby union), northampto...</td>\n",
       "      <td>0.074566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(football league championship, football league...</td>\n",
       "      <td>0.077455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>(communes of the allier department, france)</td>\n",
       "      <td>0.067757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>(english premiership (rugby union), harlequin ...</td>\n",
       "      <td>0.069674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>(communes of the allier department, department...</td>\n",
       "      <td>0.067415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>(japan national football team, list of japan i...</td>\n",
       "      <td>0.099089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>(latvia, list of settlements in latvia)</td>\n",
       "      <td>0.058670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>(communes of france, communes of the aisne dep...</td>\n",
       "      <td>0.061071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>(communes of the pas-de-calais department, dep...</td>\n",
       "      <td>0.060826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>(communes of the yonne department, yonne)</td>\n",
       "      <td>0.068315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>(aquitaine, communes of the gironde department)</td>\n",
       "      <td>0.060398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>(english premiership (rugby union), saracens f...</td>\n",
       "      <td>0.072844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>(english premiership (rugby union), leicester ...</td>\n",
       "      <td>0.078618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>(communes of the aisne department, picardie)</td>\n",
       "      <td>0.061068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>(communes of france, communes of the mayenne d...</td>\n",
       "      <td>0.064315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(english premiership (rugby union), exeter chi...</td>\n",
       "      <td>0.080833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>(communes of france, communes of the yonne dep...</td>\n",
       "      <td>0.068311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>(communes of the calvados department, departme...</td>\n",
       "      <td>0.059933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(football league one, football league two)</td>\n",
       "      <td>0.073558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>(communes of the pyr\\u00e9n\\u00e9es-atlantique...</td>\n",
       "      <td>0.068244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>(communes of france, communes of the vend\\u00e...</td>\n",
       "      <td>0.067288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>(communes of the pas-de-calais department, nor...</td>\n",
       "      <td>0.060821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>(communes of the yonne department, france)</td>\n",
       "      <td>0.068618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>(bath rugby, english premiership (rugby union))</td>\n",
       "      <td>0.073916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(communes of the mayenne department, france)</td>\n",
       "      <td>0.064823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(english premiership (rugby union), newcastle ...</td>\n",
       "      <td>0.064769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(communes of france, communes of the allier de...</td>\n",
       "      <td>0.067215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(commune in france, communes of the pyr\\u00e9n...</td>\n",
       "      <td>0.068198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    k         v\n",
       "0            (list of cities in idaho, united states)  0.056029\n",
       "1   (communes of the alpes-maritimes department, f...  0.056114\n",
       "2   (communes of france, communes of the vaucluse ...  0.057264\n",
       "3                  (city, list of cities in oklahoma)  0.057611\n",
       "4                    (idaho, list of cities in idaho)  0.056152\n",
       "5               (ain, communes of the ain department)  0.058061\n",
       "6     (communes of the vaucluse department, vaucluse)  0.057575\n",
       "7        (list of settlements in latvia, town rights)  0.058073\n",
       "8   (communes of the vaucluse department, provence...  0.057638\n",
       "9   (communes of the alpes-maritimes department, p...  0.057565\n",
       "10        (list of cities in arkansas, united states)  0.056776\n",
       "11  (communes of the ain department, regions of fr...  0.058120\n",
       "12             (list of cities in oklahoma, oklahoma)  0.058706\n",
       "13            (list of cities in iowa, united states)  0.060531\n",
       "14  (communes of the sarthe department, department...  0.059162\n",
       "15        (communes of the sarthe department, sarthe)  0.059004\n",
       "16  (communes of the sarthe department, pays de la...  0.059150\n",
       "17        (j. league division 1, japan soccer league)  0.058856\n",
       "18  (communes of the ain department, departments o...  0.058213\n",
       "19        (list of cities in oklahoma, united states)  0.058748\n",
       "20      (communes of the gironde department, gironde)  0.060302\n",
       "21                  (commune in romania, dolj county)  0.058028\n",
       "22      (communes of the vaucluse department, france)  0.058470\n",
       "23           (communes of the ain department, france)  0.058298\n",
       "24  (communes of the sarthe department, regions of...  0.059029\n",
       "25  (communes of the ain department, rh\\u00f4ne-al...  0.058738\n",
       "26  (communes of the calvados department, regions ...  0.059942\n",
       "27        (allier, communes of the allier department)  0.067469\n",
       "28  (communes of the pas-de-calais department, pas...  0.060812\n",
       "29  (communes of the gironde department, departmen...  0.060407\n",
       "..                                                ...       ...\n",
       "70  (english premiership (rugby union), gloucester...  0.082351\n",
       "71  (communes of the pyr\\u00e9n\\u00e9es-atlantique...  0.068309\n",
       "72  (english premiership (rugby union), northampto...  0.074566\n",
       "73  (football league championship, football league...  0.077455\n",
       "74        (communes of the allier department, france)  0.067757\n",
       "75  (english premiership (rugby union), harlequin ...  0.069674\n",
       "76  (communes of the allier department, department...  0.067415\n",
       "77  (japan national football team, list of japan i...  0.099089\n",
       "78            (latvia, list of settlements in latvia)  0.058670\n",
       "79  (communes of france, communes of the aisne dep...  0.061071\n",
       "80  (communes of the pas-de-calais department, dep...  0.060826\n",
       "81          (communes of the yonne department, yonne)  0.068315\n",
       "82    (aquitaine, communes of the gironde department)  0.060398\n",
       "83  (english premiership (rugby union), saracens f...  0.072844\n",
       "84  (english premiership (rugby union), leicester ...  0.078618\n",
       "85       (communes of the aisne department, picardie)  0.061068\n",
       "86  (communes of france, communes of the mayenne d...  0.064315\n",
       "87  (english premiership (rugby union), exeter chi...  0.080833\n",
       "88  (communes of france, communes of the yonne dep...  0.068311\n",
       "89  (communes of the calvados department, departme...  0.059933\n",
       "90         (football league one, football league two)  0.073558\n",
       "91  (communes of the pyr\\u00e9n\\u00e9es-atlantique...  0.068244\n",
       "92  (communes of france, communes of the vend\\u00e...  0.067288\n",
       "93  (communes of the pas-de-calais department, nor...  0.060821\n",
       "94         (communes of the yonne department, france)  0.068618\n",
       "95    (bath rugby, english premiership (rugby union))  0.073916\n",
       "96       (communes of the mayenne department, france)  0.064823\n",
       "97  (english premiership (rugby union), newcastle ...  0.064769\n",
       "98  (communes of france, communes of the allier de...  0.067215\n",
       "99  (commune in france, communes of the pyr\\u00e9n...  0.068198\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.9\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def double_link_stats():\n",
    "    return answer\n",
    "\n",
    "grader.score(question_name='mr__double_link', func=double_link_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
